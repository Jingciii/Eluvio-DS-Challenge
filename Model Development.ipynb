{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import uniform, randint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load caches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleandata.csv')\n",
    "label = pd.read_csv('./data/label.csv')\n",
    "\n",
    "train_test_split = joblib.load(\"./cache/train_test_split.joblib\")\n",
    "meta_feat = joblib.load('./cache/meta_feat.joblib')\n",
    "text_feat = joblib.load('./cache/text_feat.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./cache/features.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.concatenate((meta_feat, text_feat))\n",
    "joblib.dump(features, './cache/features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, val_train, val_test = train_test_split(df, label['up_votes'], test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, col):\n",
    "    '''\n",
    "    standardize a list of columns\n",
    "    Arg: \n",
    "        df: pd.DataFrame\n",
    "        col: list\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df[col])).set_index(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingciwang/anaconda3/envs/eluvio/lib/python3.7/site-packages/pandas/core/frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "/Users/jingciwang/anaconda3/envs/eluvio/lib/python3.7/site-packages/pandas/core/frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "df_train[['yearTonow', 'word_count']] = standardize(df_train, ['yearTonow', 'word_count'])\n",
    "df_test[['yearTonow', 'word_count']] = standardize(df_test, ['yearTonow', 'word_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get meta feature matrix\n",
    "meta_train = scipy.sparse.csr_matrix(df_train[meta_feat])\n",
    "meta_test = scipy.sparse.csr_matrix(df_test[meta_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_matrix(df, col='title_clean', vocabulary=text_feat):\n",
    "\n",
    "    vec = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vec.fit_transform(df[col])\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text feature matrix\n",
    "text_train = get_text_matrix(df_train)\n",
    "text_test = get_text_matrix(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features matrix\n",
    "m_train = hstack([meta_train, text_train])\n",
    "m_test = hstack([meta_test, text_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that this is a regression problem, intuitively *Linear Regression* model should be sufficient to serve as benchmark model. Since it's not common for news title to have repeated words, most features are sort of binary variable. Therefore, I would expect tree-based models work for this problem.\n",
    "\n",
    "**Models**:\n",
    "\n",
    " - Liear Regression\n",
    " - Random Forest\n",
    " - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error, mean_absolute_error\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 3\n",
    "cv = KFold(n_splits=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_report(y_train, y_train_hat, y_test, y_test_hat):\n",
    "    \n",
    "    r2_train = r2_score(y_train_hat, y_train)\n",
    "    r2_test = r2_score(y_test_hat, y_test)\n",
    "    evs_train = explained_variance_score(y_train_hat, y_train)\n",
    "    evs_test = explained_variance_score(y_test_hat, y_test)\n",
    "    rmse_train = mean_squared_error(y_train_hat, y_train, squared=False)\n",
    "    rmse_test = mean_squared_error(y_test_hat, y_test, squared=False)\n",
    "    mae_train = mean_absolute_error(y_train_hat, y_train)\n",
    "    mae_test = mean_absolute_error(y_test_hat, y_test)\n",
    "    \n",
    "    print(tabulate([['trainset', rmse_train, mae_train, r2_train, evs_train], \\\n",
    "                    ['testset', rmse_test, mae_test, r2_test, evs_test]], \\\n",
    "                   headers=['Set', 'RMSE', 'MAE', 'Explained_Variance', 'R2']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   15.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score=nan,\n",
       "                   estimator=ElasticNet(alpha=1.0, copy_X=True,\n",
       "                                        fit_intercept=True, l1_ratio=0.5,\n",
       "                                        max_iter=1000, normalize=False,\n",
       "                                        positive=False, precompute=False,\n",
       "                                        random_state=None, selection='cyclic',\n",
       "                                        tol=0.0001, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a37c5fe90>,\n",
       "                                        'l1_ratio': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a37c5fe50>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False,\n",
       "                   scoring='neg_root_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = ElasticNet()\n",
    "params = dict(alpha=uniform(), l1_ratio=uniform())\n",
    "p_search = RandomizedSearchCV(lr_model, params, \n",
    "                              n_jobs=-1, random_state=42,\n",
    "                              cv=cv, verbose=True,\n",
    "                              scoring='neg_root_mean_squared_error')\n",
    "p_search.fit(m_train, val_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/ElasticNet.joblib']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('./model'):\n",
    "    os.mkdir('./model')\n",
    "joblib.dump(p_search.best_estimator_, './model/ElasticNet.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-505.00782508319315"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.020584494295802447, 'l1_ratio': 0.9699098521619943}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = p_search.best_estimator_.predict(m_train)\n",
    "y_test = p_search.best_estimator_.predict(m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set          RMSE      MAE    Explained_Variance       R2\n",
      "--------  -------  -------  --------------------  -------\n",
      "trainset  503.366  168.107              0.13505   0.13505\n",
      "testset   505.804  169.943              0.134155  0.13418\n"
     ]
    }
   ],
   "source": [
    "predict_report(y_train, val_train, y_test, val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   28.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf...\n",
       "                                                   warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a3f1e0dd0>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a3f1e0ad0>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False,\n",
       "                   scoring='neg_root_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(verbose=True)\n",
    "params = {'max_depth': randint(2, 20),\n",
    " 'n_estimators': randint(10, 50)}\n",
    "\n",
    "p_search = RandomizedSearchCV(rf_model, params, \n",
    "                              n_jobs=-1, random_state=42,\n",
    "                              cv=cv, verbose=True,\n",
    "                              scoring='neg_root_mean_squared_error')\n",
    "p_search.fit(m_train, val_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/RandomForest.joblib']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(p_search.best_estimator_, './model/RandomForest.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-506.96032672766023\n",
      "{'max_depth': 9, 'n_estimators': 33}\n"
     ]
    }
   ],
   "source": [
    "print(p_search.best_score_)\n",
    "print(p_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set          RMSE      MAE    Explained_Variance        R2\n",
      "--------  -------  -------  --------------------  --------\n",
      "trainset  498.686  160.974              0.151059  0.151061\n",
      "testset   506.704  163.597              0.131071  0.131106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "y_train = p_search.best_estimator_.predict(m_train)\n",
    "y_test = p_search.best_estimator_.predict(m_test)\n",
    "\n",
    "predict_report(y_train, val_train, y_test, val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      287810.7062            1.14m\n",
      "         2      283152.3651            1.08m\n",
      "         3      279042.1316            1.06m\n",
      "         4      275395.6226            1.02m\n",
      "         5      272125.5904            1.00m\n",
      "         6      269068.0243           58.96s\n",
      "         7      266299.2200           57.55s\n",
      "         8      263821.2993           56.18s\n",
      "         9      261598.0747           54.31s\n",
      "        10      259613.7543           52.47s\n",
      "        20      247084.9838           37.94s\n",
      "        30      240563.3046           25.06s\n",
      "        40      237366.8802           12.79s\n",
      "        50      234471.0133            1.14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score=nan,\n",
       "                   estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                       criterion='friedman_mse',\n",
       "                                                       init=None,\n",
       "                                                       learning_rate=0.1,\n",
       "                                                       loss='ls', max_depth=3,\n",
       "                                                       max_features=None,\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,...\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a37a8b110>,\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a37a8b850>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a4159f690>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False,\n",
       "                   scoring='neg_root_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = GradientBoostingRegressor(verbose=True)\n",
    "params = {'n_estimators': randint(10, 80), \n",
    "          'max_depth': randint(3, 10),\n",
    "          'learning_rate': uniform(0.01, 0.1)}\n",
    "p_search = RandomizedSearchCV(xgb_model, params, \n",
    "                              n_jobs=-1, random_state=42,\n",
    "                              cv=cv, verbose=True,\n",
    "                              scoring='neg_root_mean_squared_error')\n",
    "p_search.fit(m_train, val_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/XGBoost.joblib']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(p_search.best_estimator_, './model/XGBoost.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-506.63399713388054\n",
      "{'learning_rate': 0.06247746602583892, 'max_depth': 9, 'n_estimators': 51}\n"
     ]
    }
   ],
   "source": [
    "print(p_search.best_score_)\n",
    "print(p_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set          RMSE      MAE    Explained_Variance        R2\n",
      "--------  -------  -------  --------------------  --------\n",
      "trainset  483.941  159.032              0.200518  0.200518\n",
      "testset   505.191  163.7                0.136253  0.136286\n"
     ]
    }
   ],
   "source": [
    "y_train = p_search.best_estimator_.predict(m_train)\n",
    "y_test = p_search.best_estimator_.predict(m_test)\n",
    "\n",
    "predict_report(y_train, val_train, y_test, val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, *Tree-based models* have much better performance on train set than linear regression. But after several rounds of hyper-parameter tuning, both *Random Forest* and *XGBoost* have inclination to overfitting and parameters like *tree-depth* and *number of estimator* have to be set relatively small. *Linear Regression* have least difference between error on train set and that on test, after tuning the hyper-parameter, larger ratio of l1 regularizer would give better performance.\n",
    "\n",
    "All three models have similar outputs on test set, but that of *XGBoost* is sightly better than the other two models. Considering the scale of the dataset, the features selected are capable of predicting up_votes to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eluvio",
   "language": "python",
   "name": "eluvio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
